좋아요 👍
이제 3부 **“CLI로 웹 정보 수집하기 (스크래핑 중심)”**를 완전한 형태로 정리해드릴게요.
실제 강의용 Markdown 형식이며, 나중에 BigKinds 외 다른 예시를 추가할 수 있도록 확장 구조까지 포함했습니다.

⸻


# 3부 — CLI로 웹 정보 수집하기 (스크래핑 중심) 🌐

---

## 1. 주제 개요

> “이제는 AI뿐만 아니라,  
> 명령어로 직접 웹 데이터를 가져올 수 있습니다.”

Claude Code는 AI를 CLI 안으로 가져온 도구였다면,  
이번에는 **CLI 자체를 활용해서 웹 데이터를 다루는 방법**을 배웁니다.  

CLI는 단순히 로컬 파일 조작 도구가 아니라,  
**웹과 서버를 직접 연결하는 강력한 인터페이스**입니다.

---

## 2. 학습 목표

- `curl` 명령으로 웹 데이터 요청하기  
- `grep`, `jq`로 필요한 부분만 추출하기  
- CLI 파이프(`|`), 리다이렉션(`>`, `>>`)의 실제 활용  
- Claude Code와 연동하여 자동 요약하기  
- GitHub CLI를 이용해 **스크래핑 결과를 웹에 배포하기**

---

## 3. 기본 명령어 정리

| 명령어 | 역할 | 예시 |
|---------|------|------|
| **curl** | 웹 요청 (HTTP GET/POST) | `curl https://example.com` |
| **grep** | 텍스트에서 특정 단어 검색 | `grep "AI" news.json` |
| **jq** | JSON 데이터 파싱/정제 | `cat news.json | jq '.resultList[].TITLE'` |
| **| (파이프)** | 명령 결과를 다음 명령의 입력으로 전달 | `cat a.txt | grep "error"` |
| **> (리다이렉션)** | 결과를 새 파일로 저장 | `echo "data" > log.txt` |
| **>> (추가 저장)** | 기존 파일에 이어쓰기 | `echo "추가" >> log.txt` |

---

## 4. 스크래핑 실습 ① — BigKinds 뉴스 데이터 📰

> 💡 BigKinds는 한국언론진흥재단이 운영하는 뉴스 데이터 서비스입니다.

### (1) 뉴스 검색 요청
```bash
curl "https://www.bigkinds.or.kr/api/news/search.do?q=AI" > news.json

→ news.json 파일이 생성됩니다.

⸻

(2) 결과 확인

cat news.json | jq '.resultList | length'
cat news.json | jq '.resultList[].TITLE'

첫 번째 명령은 기사 개수를,
두 번째 명령은 기사 제목만 출력합니다.

⸻

(3) 결과 저장

cat news.json | jq '.resultList[].TITLE' > news_titles.txt
head news_titles.txt

결과를 파일로 저장한 뒤, 일부 내용을 미리 확인합니다.

⸻

5. 스크래핑 실습 ② — 추가 예제 자리

이 부분에는 BigKinds 외의 실제 예시를 추가할 수 있습니다.

예시 제안:
	•	네이버 뉴스 검색 결과
	•	공공데이터포털 OpenAPI
	•	위키백과 요약 페이지
	•	구글 뉴스 RSS 피드

(추후 실제 curl 명령 삽입 예정)

⸻

6. CLI 데이터 다루기 실습 🧩

# 특정 키워드 필터링
grep "AI" news.json > filtered.txt

# JSON에서 특정 필드 추출
cat news.json | jq '.resultList[].CONTENT' > content.txt

# 두 명령을 연결해서 결과만 추출
cat news.json | jq '.resultList[].TITLE' | grep "경제" > economy_titles.txt

💡 핵심 포인트

“명령어를 연결하면, 데이터 가공 과정이 하나의 ‘파이프라인’이 된다.”

⸻

7. Claude Code와 결합하기 🤝

Claude Code가 실행 중이라면 이렇게 말할 수 있습니다:

news_titles.txt 파일의 내용을 요약해서  
AI 뉴스의 핵심 트렌드를 summary.txt에 정리해줘.

CLI로 데이터를 모으고, Claude로 분석 및 요약을 맡기는 협업 구조입니다.

⸻

8. 결과를 GitHub Pages에 배포하기 🚀

(1) 기존 방식 (복잡한 설정)
	1.	GitHub 접속 → 새 리포지토리 생성
	2.	로컬에서 다음 명령 수행:

git init
git add .
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/USERNAME/demo.git
git push -u origin main


	3.	GitHub → Settings → Pages → main 브랜치 선택

단계가 많고, 초보자에겐 헷갈립니다.

⸻

(2) GitHub CLI (gh)로 간단하게

CLI로 모든 과정을 한 번에 처리할 수 있습니다.

# 리포지토리 생성
gh repo create scraping-demo --public --source=.

# 커밋 및 푸시
git add .
git commit -m "add scraped news data"
git push origin main

# GitHub Pages 활성화
gh repo edit --enable-pages

💡 포인트

“이제 브라우저를 열지 않아도, CLI 한 줄로 배포가 가능합니다.”

⸻

(3) 사이트 열기

open "https://USERNAME.github.io/scraping-demo"

스크래핑한 데이터를 웹페이지로 바로 확인할 수 있습니다.

⸻

9. 확장 예시 — Claude Code 자동화

Claude Code와 gh CLI를 함께 사용할 수도 있습니다:

BigKinds에서 AI 관련 뉴스를 10개 스크랩해서  
news.html로 정리하고,  
GitHub Pages로 자동 배포해줘.

Claude Code는 내부적으로 curl, jq, gh 명령을 조합하여
스크래핑 → 요약 → 배포의 전 과정을 자동화할 수 있습니다.

⸻

10. 핵심 메시지

“스크래핑은 데이터를 ‘가져오는 기술’이고,
CLI는 그 데이터를 ‘흐르게 하는 기술’입니다.”

“AI 없이도, AI처럼 일할 수 있다.”
이제 우리는 명령어만으로 데이터를 수집하고,
세상에 배포할 수 있습니다.

---

## 💡 강의 흐름 요약

| 단계 | 실습 내용 | 주요 명령어 |
|------|------------|-------------|
| 1️⃣ | CLI 개념 및 준비 | `curl`, `jq`, `grep` |
| 2️⃣ | BigKinds 데이터 스크래핑 | `curl` 요청, JSON 저장 |
| 3️⃣ | 데이터 가공 실습 | `jq`, `grep`, 파이프라인 |
| 4️⃣ | Claude Code와 협업 | 자동 요약, 파일 생성 |
| 5️⃣ | GitHub Pages 배포 | `gh` CLI |
| 6️⃣ | 전체 자동화 확장 | Claude Code 통합 명령 |

---

이 3부 구조는 **데이터 수집 → 가공 → 배포**의 전체 흐름을 보여주기 때문에,  
4부의 “에이전트 만들기(자동화)”로 완벽하게 연결됩니다.  

원하신다면 다음 단계로,  
이 3부에서 사용할 **실제 BigKinds API + 네이버 뉴스 + 위키백과 스크래핑 예시 코드 세트**를 만들어드릴까요?  
(`curl` + `jq`만으로 동작 가능한 형태로요.)